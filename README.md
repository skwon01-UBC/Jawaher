<div align="center">

<h2 class="papername">
  <img src="./assets/Jawaher_Logo.png" style="vertical-align: -10px;" height="70px" width="70px">
  Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking
</h2>

<div>
  <!-- Optionally, add team or contributor links here -->
  <!-- For example:
  <div>
      <a href="https://your-institution.edu/team" target="_blank">Your Name</a>,
      <a href="https://your-institution.edu/team" target="_blank">Collaborator Name</a>
  </div>
  -->
  Your Institution or Affiliation<br>
</div>

<p align="center">
  <a href='https://arxiv.org/abs/2503.00231'><img src='https://img.shields.io/badge/Paper-Arxiv-red'></a>
  <a href='https://huggingface.co/datasets/UBC-NLP/Jawaher-benchmark'><img src='https://img.shields.io/badge/Dataset-HuggingFace-green'></a>
  <a href='#'><img src='https://img.shields.io/badge/Publication-NAACL 2025-yellow'></a>
</p>

:fire: Details will be released. Stay tuned :beers: :+1:

</div>

## Updates
- **[03/01/2025]** We've released the Jawaher dataset: [Dataset](https://huggingface.co/datasets/UBC-NLP/Jawaher-benchmark)
- **[01/22/2025]** [Jawaher](#) has been accepted at [NAACL 2025].
- **[03/03/2025]** [ArXiv paper](#) released.

## Abstract
Recent advancements in instruction fine-tuning, alignment methods such as reinforcement learning from human feedback (RLHF), and optimization techniques like direct preference optimization (DPO), have significantly enhanced the adaptability of large language models (LLMs) to user preferences. However, despite these innovations, many LLMs continue to exhibit biases toward Western, Anglo-centric, or American cultures, with performance on English data consistently surpassing that of other languages. This reveals a persistent cultural gap in LLMs, which complicates their ability to accurately process culturally rich and diverse figurative language, such as proverbs. To address this, we introduce Jawaher, a benchmark designed to assess LLMs' capacity to comprehend and interpret Arabic proverbs. Jawaher includes proverbs from various Arabic dialects, along with idiomatic translations and explanations. Through extensive evaluations of both open- and closed-source models, we find that while LLMs can generate idiomatically accurate translations, they struggle with producing culturally nuanced and contextually relevant explanations. These findings highlight the need for ongoing model refinement and dataset expansion to bridge the cultural gap in figurative language processing. Project GitHub page is accessible at: https://github.com/UBC-NLP/jawaher

## Citation

If you find this work useful for your research, please kindly cite our paper:
```bibtex
@inproceedings{yourname2024jawaher,
    title = "Jawaher: A Multidialectal Dataset of Arabic Proverbs for LLM Benchmarking",
    author = "Your Name and Collaborators",
    booktitle = "Proceedings of the [Conference Name]",
    year = "2024",
    address = "[Conference Location]",
    publisher = "[Publisher]",
    url = "[Paper URL]",
    doi = "[DOI]",
    pages = "[Pages]"
}
